# ReFitd-ETL Checklist Progress

**Saved:** 2026-02-21 05:40 UTC  
**Resume:** Pick up from TASK 2

---

## âœ… COMPLETED

### TASK 1: Supabase Setup âœ…
- Created new Supabase project: `refitd-etl`
- **Project URL:** `https://rjdjmkujvyofxqfjlgqi.supabase.co`
- **Keys configured in `.env`** (service_role key works)
- **All tables created:** products, curation_history, curated_metadata, curation_status, ai_generated_tags, custom_vocabulary
- **RLS policies applied:** Anon=read-only, Authenticated=full access
- Upgraded supabase-py to v2.28.0 (supports new `sb_secret_` key format)

### GitHub Repo Setup âœ…
- Repo: https://github.com/trevsauer/refitd-etl
- README with full documentation pushed
- Security SQL file committed

---

## ðŸ”¶ IN PROGRESS

### TASK 2: Test Curation Workflow
**Status:** Need to scrape products first, then curate

**Next Steps:**
1. Run scraper to populate database:
   ```bash
   cd /Users/sauerbreytrevor/Downloads/refitd-scraper/refitd-scraper
   source venv/bin/activate
   python main.py --sample-all --no-images
   ```

2. Start the viewer/curation UI:
   ```bash
   python viewer.py
   # Open http://127.0.0.1:5001
   ```

3. Curate 3-5 products with fake feedback:
   - Remove at least 1 tag (with reason)
   - Add at least 1 tag (with reason)
   - Change at least 1 tag value (with reason)
   - Check error type boxes
   - Set confidence rating
   - Click "Mark as Complete"

4. Verify in Supabase SQL Editor:
   ```sql
   SELECT product_id, name,
          tags_final -> 'deleted_tags' as deleted_tags,
          tags_final -> 'added_tags' as added_tags
   FROM products
   WHERE curated_at IS NOT NULL
   LIMIT 5;
   ```

---

## â³ PENDING

### TASK 3: Export Training Data
```bash
cd /Users/sauerbreytrevor/Downloads/refitd-scraper/refitd-scraper
source venv/bin/activate
python scripts/export_training_data.py -o test_export.jsonl --limit 10
```

### TASK 4: Verify JSONL Format
```bash
python scripts/validate_training_data.py test_export.jsonl
```

### TASK 5: Wipe Database
```bash
python scripts/wipe_database.py
# Type: DELETE EVERYTHING
```

### TASK 6: Report Back
- Commit any remaining changes
- Push to GitHub
- Confirm all tasks complete

---

## ðŸ“ Key Paths

- **Repo:** `/Users/sauerbreytrevor/Downloads/refitd-scraper/refitd-scraper`
- **Venv:** `source venv/bin/activate`
- **Viewer:** `python viewer.py` â†’ http://127.0.0.1:5001
- **.env:** Already configured with valid Supabase + OpenAI keys

---

## âš ï¸ Notes

1. **Storage bucket warning:** When scraping, you'll see "Could not access bucket 'product-images'". 
   - Create bucket in Supabase Dashboard â†’ Storage â†’ New Bucket â†’ "product-images" (public)
   - Or use `--no-images` flag to skip image uploads

2. **Playwright browser:** Already installed (chromium)

3. **Database is empty:** Products need to be scraped before curation testing

---

## Resume Command

When you're back, just say:
> "Resume the refitd-etl checklist from Task 2"

And I'll pick up right where we left off!
