"""
Zara product extractor using Playwright with stealth settings.
Handles JavaScript rendering and anti-bot detection.
"""

import asyncio
import random
import re
import sys
from dataclasses import dataclass, field
from datetime import datetime
from typing import Optional

from playwright.async_api import async_playwright, Browser, BrowserContext, Page
from playwright_stealth import stealth_async
from rich.console import Console

sys.path.insert(0, str(__file__).rsplit("/", 3)[0])
from config.settings import config, ScraperConfig

console = Console()


def slugify_color(color_name: str) -> str:
    """Convert a color name to a URL-safe slug for product IDs."""
    # Convert to lowercase, replace spaces and special chars with underscore
    slug = color_name.lower()
    slug = re.sub(r"[^a-z0-9]+", "_", slug)
    slug = slug.strip("_")
    return slug or "unknown"


@dataclass
class RawProductData:
    """Raw product data extracted from Zara."""

    product_id: str
    name: str
    url: str
    category: str
    price_current: Optional[float] = None
    price_original: Optional[float] = None
    currency: str = "USD"
    description: Optional[str] = None
    colors: list = field(default_factory=list)  # All available colors (for reference)
    color: Optional[str] = None  # Single color for this variant (if expanded)
    parent_product_id: Optional[str] = (
        None  # Original product ID if this is a color variant
    )
    sizes: list = field(default_factory=list)
    materials: list = field(default_factory=list)
    image_urls: list = field(default_factory=list)
    fit: Optional[str] = None  # slim, relaxed, wide, regular, etc.
    weight: Optional[str] = None  # light, medium, heavy
    composition: Optional[str] = None  # e.g., "100% cotton" - legacy string format
    composition_structured: Optional[dict] = (
        None  # Hierarchical composition data: {"parts": [{"name": "OUTER SHELL", "areas": [...]}]}
    )
    scraped_at: str = field(default_factory=lambda: datetime.utcnow().isoformat() + "Z")


class ZaraExtractor:
    """Extracts product data from Zara website using Playwright."""

    def __init__(
        self,
        scraper_config: Optional[ScraperConfig] = None,
        browser_type: str = "firefox",
    ):
        self.config = scraper_config or config.scraper
        self.browser: Optional[Browser] = None
        self.context: Optional[BrowserContext] = None
        self.playwright = None
        self.browser_type = browser_type  # "firefox", "chromium", or "webkit"

    async def __aenter__(self):
        """Async context manager entry."""
        await self.start()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit."""
        await self.close()

    async def start(self) -> None:
        """Start the browser with stealth settings."""
        console.print(f"[bold blue]Starting {self.browser_type} browser...[/bold blue]")

        self.playwright = await async_playwright().start()

        # Use Firefox by default (more stable on macOS ARM)
        # Fall back to other browsers if needed
        browser_launchers = {
            "firefox": self.playwright.firefox,
            "chromium": self.playwright.chromium,
            "webkit": self.playwright.webkit,
        }

        launcher = browser_launchers.get(self.browser_type, self.playwright.firefox)

        try:
            self.browser = await launcher.launch(
                headless=self.config.headless,
            )
        except Exception as e:
            console.print(f"[yellow]Failed to launch {self.browser_type}: {e}[/yellow]")
            console.print("[yellow]Trying Firefox as fallback...[/yellow]")
            self.browser = await self.playwright.firefox.launch(
                headless=self.config.headless,
            )

        # Firefox user agents
        firefox_user_agents = [
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:121.0) Gecko/20100101 Firefox/121.0",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
        ]

        user_agent = (
            random.choice(firefox_user_agents)
            if self.browser_type == "firefox"
            else random.choice(self.config.user_agents)
        )

        self.context = await self.browser.new_context(
            viewport={
                "width": self.config.viewport_width,
                "height": self.config.viewport_height,
            },
            user_agent=user_agent,
            locale="en-US",
            timezone_id="America/New_York",
        )

        console.print("[bold green]Browser started successfully[/bold green]")

    async def close(self) -> None:
        """Close the browser."""
        if self.context:
            await self.context.close()
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()
        console.print("[bold blue]Browser closed[/bold blue]")

    async def _create_stealth_page(self) -> Page:
        """Create a new page with stealth settings."""
        page = await self.context.new_page()
        await stealth_async(page)
        return page

    async def _random_delay(self, base_delay: Optional[float] = None) -> None:
        """Add a random delay to mimic human behavior."""
        delay = base_delay or self.config.page_delay_seconds
        jitter = random.uniform(0.5, 1.5)
        await asyncio.sleep(delay * jitter)

    async def get_category_product_urls(
        self, category_key: str, limit: Optional[int] = None
    ) -> list[str]:
        """
        Get product URLs from a category page, handling pagination.

        Uses Zara's viewPayload data structure which contains all product info
        including products not yet rendered in the DOM (virtual rendering).

        Args:
            category_key: Key from config categories (e.g., 'tshirts', 'pants', 'jackets')
            limit: Maximum number of product URLs to return

        Returns:
            List of product URLs
        """
        limit = limit or self.config.products_per_category
        category_path = self.config.categories.get(category_key)

        if not category_path:
            console.print(f"[bold red]Unknown category: {category_key}[/bold red]")
            return []

        base_url = f"{self.config.base_url}{category_path}"
        console.print(f"[cyan]Fetching category: {category_key} from {base_url}[/cyan]")

        # Extract expected category ID from URL for redirect validation
        # URL format: /us/en/man-category-l###.html or ...l###.html?params
        expected_category_id = None
        if "-l" in category_path:
            raw = category_path.split("-l")[-1].replace(".html", "")
            expected_category_id = raw.split("?")[0].strip() or None

        all_product_links = {}  # Use dict: product_id -> url to avoid duplicates
        current_page = 1
        max_pages = 20  # Safety limit

        while current_page <= max_pages and len(all_product_links) < limit:
            # Build URL with pagination parameter (use & if URL already has query params)
            if current_page == 1:
                url = base_url
            else:
                sep = "&" if "?" in base_url else "?"
                url = f"{base_url}{sep}page={current_page}"

            page = await self._create_stealth_page()

            try:
                await page.goto(
                    url, wait_until="networkidle", timeout=self.config.timeout_ms
                )
                await self._random_delay()

                # Check for redirects - Zara reuses category IDs across sections
                final_url = page.url
                if (
                    expected_category_id
                    and f"-l{expected_category_id}" not in final_url
                ):
                    console.print(f"[bold yellow]âš  Redirect detected![/bold yellow]")
                    console.print(f"[yellow]  Expected: {base_url}[/yellow]")
                    console.print(f"[yellow]  Got: {final_url}[/yellow]")
                    console.print(
                        f"[yellow]  Category URL may be outdated. Skipping to avoid wrong products.[/yellow]"
                    )
                    await page.close()
                    return []  # Return empty to indicate category needs URL update

                # Extract product URLs from viewPayload (Zara's internal data structure)
                # This is more reliable than DOM extraction due to virtual rendering
                payload_data = await page.evaluate(
                    """
                    () => {
                        if (!window.zara || !window.zara.viewPayload) return null;

                        const payload = window.zara.viewPayload;
                        const baseUrl = window.location.origin;
                        const products = [];

                        // Extract product URLs from productGroups
                        if (payload.productGroups) {
                            payload.productGroups.forEach(group => {
                                if (group.elements) {
                                    group.elements.forEach(element => {
                                        if (element.commercialComponents) {
                                            element.commercialComponents.forEach(comp => {
                                                if (comp.seo && comp.seo.seoProductId && comp.seo.keyword) {
                                                    const url = baseUrl + '/us/en/' + comp.seo.keyword + '-p' + comp.seo.seoProductId + '.html';
                                                    products.push({
                                                        id: comp.seo.seoProductId,
                                                        url: url
                                                    });
                                                }
                                            });
                                        }
                                    });
                                }
                            });
                        }

                        return {
                            page: payload.paginationInfo?.page || 1,
                            isLastPage: payload.paginationInfo?.isLastPage || false,
                            products: products
                        };
                    }
                    """
                )

                # Fallback to DOM extraction if viewPayload not available
                if not payload_data:
                    console.print(
                        f"[yellow]  viewPayload not found, falling back to DOM extraction...[/yellow]"
                    )
                    # Legacy DOM-based extraction
                    if limit > 20:
                        await self._scroll_to_load_all_products(page)
                    else:
                        await self._scroll_page(page, scroll_count=5)

                    page_links = await page.evaluate(
                        """
                        () => {
                            const links = [];
                            const selectors = [
                                'a.product-link',
                                'a[href*="-p"][href$=".html"]',
                                '.product-grid-product a',
                                '[data-productid] a'
                            ];

                            for (const selector of selectors) {
                                const elements = document.querySelectorAll(selector);
                                for (const el of elements) {
                                    const href = el.href;
                                    if (href && href.includes('-p') && href.endsWith('.html') && !links.includes(href)) {
                                        links.push(href);
                                    }
                                }
                                if (links.length > 0) break;
                            }
                            return links;
                        }
                        """
                    )

                    links_before = len(all_product_links)
                    for link in page_links:
                        # Extract product ID from URL
                        match = re.search(r"-p(\d+)\.html", link)
                        if match:
                            all_product_links[match.group(1)] = link

                    new_links = len(all_product_links) - links_before
                    console.print(
                        f"[dim]  Page {current_page}: Found {len(page_links)} products ({new_links} new) [DOM][/dim]"
                    )
                    # No pagination info available in fallback mode
                    break
                else:
                    # Process viewPayload data
                    links_before = len(all_product_links)
                    for product in payload_data.get("products", []):
                        if product["id"] not in all_product_links:
                            all_product_links[product["id"]] = product["url"]

                    new_links = len(all_product_links) - links_before
                    console.print(
                        f"[dim]  Page {current_page}: Found {len(payload_data.get('products', []))} products ({new_links} new)[/dim]"
                    )

                    # Check if this is the last page
                    if payload_data.get("isLastPage", True):
                        break

                current_page += 1
                await self._random_delay()

            except Exception as e:
                console.print(
                    f"[bold red]Error fetching category page {current_page}: {e}[/bold red]"
                )
                break
            finally:
                await page.close()

        product_links = list(all_product_links.values())
        console.print(
            f"[green]Found {len(product_links)} total products in {category_key} (across {current_page} page(s))[/green]"
        )
        return product_links[:limit]

    async def _scroll_page(self, page: Page, scroll_count: int = 3) -> None:
        """Scroll page to trigger lazy loading."""
        for i in range(scroll_count):
            await page.evaluate("window.scrollBy(0, window.innerHeight)")
            await asyncio.sleep(0.5)

    async def _scroll_to_load_all_products(
        self, page: Page, max_scrolls: int = 100
    ) -> int:
        """
        Scroll page until all products are loaded (infinite scroll).

        This scrolls to the bottom repeatedly until no new products appear
        for 5 consecutive scrolls. Also clicks "See more" or "View all" buttons
        if found.

        Args:
            page: Playwright page
            max_scrolls: Maximum number of scroll attempts (safety limit)

        Returns:
            Number of products found
        """
        previous_count = 0
        same_count_times = 0
        scroll_attempts = 0

        while same_count_times < 5 and scroll_attempts < max_scrolls:
            # First, try to click any "See more" or "View all" buttons
            try:
                clicked = await page.evaluate(
                    """
                    () => {
                        // Look for "See more", "View all", "Load more" buttons
                        const buttons = document.querySelectorAll('button, a');
                        for (const btn of buttons) {
                            const text = btn.textContent.toLowerCase().trim();
                            if (text === 'see more' || text === 'view all' || text === 'load more' || text === 'show more') {
                                // Check if it's visible and in the product listing area (not footer)
                                const rect = btn.getBoundingClientRect();
                                if (rect.top > 0 && rect.top < window.innerHeight * 2) {
                                    btn.click();
                                    return true;
                                }
                            }
                        }
                        return false;
                    }
                    """
                )
                if clicked:
                    console.print(f"[dim]  ... clicked 'See more' button[/dim]")
                    await asyncio.sleep(2)  # Wait for new products to load
            except Exception:
                pass

            # Scroll to bottom of page
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await asyncio.sleep(1.5)  # Wait for lazy loading (increased from 1s)
            scroll_attempts += 1

            # Count unique product links
            count = await page.evaluate(
                """
                () => {
                    const products = new Set();
                    document.querySelectorAll('a').forEach(link => {
                        const match = link.href.match(/-p(\\d+)\\.html/);
                        if (match) products.add(match[1]);
                    });
                    return products.size;
                }
                """
            )

            if count == previous_count:
                same_count_times += 1
            else:
                same_count_times = 0
                previous_count = count
                console.print(f"[dim]  ... loaded {count} products[/dim]")

        return previous_count

    async def extract_product(
        self, url: str, category: str
    ) -> Optional[RawProductData]:
        """
        Extract product data from a product page.

        Args:
            url: Product page URL
            category: Category name for organization

        Returns:
            RawProductData object or None if extraction fails
        """
        console.print(f"[cyan]Extracting product: {url}[/cyan]")

        # Extract product ID from URL first
        product_id = self._extract_product_id(url)

        # Try to get data from ITXRest API first (more reliable)
        api_data = await self._get_product_from_api(product_id)

        page = await self._create_stealth_page()

        try:
            await page.goto(
                url, wait_until="networkidle", timeout=self.config.timeout_ms
            )
            await self._random_delay(2.0)

            # Extract product name - try API data first, then DOM
            name = None
            if api_data and api_data.get("name"):
                name = api_data["name"]
            else:
                name = await self._extract_text(
                    page,
                    [
                        "h1.product-detail-info__header-name",
                        'h1[class*="product-name"]',
                        ".product-detail-info h1",
                        "h1",
                    ],
                )

            # Validate: Skip if we still don't have a valid name
            if not name or name == "Unknown" or len(name) < 2:
                # Try to extract name from URL as last resort
                url_name = self._extract_name_from_url(url)
                if url_name:
                    name = url_name
                else:
                    console.print(
                        f"[yellow]âš  Skipping product {product_id}: Could not extract name[/yellow]"
                    )
                    return None

            # Extract prices - try API first, then DOM
            price_current = None
            price_original = None
            if api_data and api_data.get("price"):
                price_current = api_data["price"]
                price_original = api_data.get("original_price")
            else:
                price_current, price_original = await self._extract_prices(page)

            # Extract description
            description = None
            if api_data and api_data.get("description"):
                description = api_data["description"]
            else:
                description = await self._extract_text(
                    page,
                    [
                        ".expandable-text__inner-content p",
                        ".product-detail-description p",
                        '[class*="description"] p',
                    ],
                )

            # Extract colors - try API first
            colors = []
            if api_data and api_data.get("colors"):
                colors = api_data["colors"]
            else:
                colors = await self._extract_colors(page)

            # Extract sizes (uses API internally)
            sizes = await self._extract_sizes(page)

            # Extract materials/composition
            materials = await self._extract_materials(page)

            # Extract composition (fabric percentage breakdown) - returns (string, structured)
            composition, composition_structured = await self._extract_composition(
                page, product_id, category=category
            )

            # Extract image URLs - try API first, then DOM (keep ALL for tagging; storage limit applied in pipeline)
            image_urls = []
            if api_data and api_data.get("images"):
                image_urls = api_data["images"]
            else:
                image_urls = await self._extract_images(page)

            # Final validation: Must have either images OR a valid price
            if not image_urls and not price_current:
                console.print(
                    f"[yellow]âš  Skipping product {product_id}: No images and no price (likely blocked)[/yellow]"
                )
                return None

            product_data = RawProductData(
                product_id=product_id,
                name=name,
                url=url,
                category=category,
                price_current=price_current,
                price_original=price_original,
                description=description,
                colors=colors,
                sizes=sizes,
                materials=materials,
                image_urls=image_urls,
                composition=composition,
                composition_structured=composition_structured,
            )

            console.print(f"[green]âœ“ Extracted: {name} ({product_id})[/green]")
            return product_data

        except Exception as e:
            console.print(f"[bold red]Error extracting product {url}: {e}[/bold red]")
            return None
        finally:
            await page.close()

    async def extract_products_by_color(
        self, url: str, category: str
    ) -> list[RawProductData]:
        """
        Extract product data from a product page, creating separate entries for each color variant.

        Args:
            url: Product page URL
            category: Category name for organization

        Returns:
            List of RawProductData objects, one per color variant.
            Returns empty list if extraction fails.
        """
        console.print(f"[cyan]Extracting product with color variants: {url}[/cyan]")

        # Extract product ID from URL first
        base_product_id = self._extract_product_id(url)

        # Get ALL color variants from API
        api_data = await self._get_product_all_colors_from_api(base_product_id)

        if not api_data or not api_data.get("color_variants"):
            # Fall back to single product extraction if API fails
            console.print(
                f"[dim]Could not get color variants from API, falling back to single product[/dim]"
            )
            single_product = await self.extract_product(url, category)
            return [single_product] if single_product else []

        page = await self._create_stealth_page()
        products = []

        try:
            await page.goto(
                url, wait_until="networkidle", timeout=self.config.timeout_ms
            )
            await self._random_delay(2.0)

            # Get shared data from DOM (materials, composition, fit)
            materials = await self._extract_materials(page)
            composition, composition_structured = await self._extract_composition(
                page, base_product_id, category=category
            )

            # Extract name (shared across all variants)
            name = api_data.get("name", "")
            if not name:
                name = await self._extract_text(
                    page,
                    [
                        "h1.product-detail-info__header-name",
                        'h1[class*="product-name"]',
                        ".product-detail-info h1",
                        "h1",
                    ],
                )

            if not name or name == "Unknown" or len(name) < 2:
                url_name = self._extract_name_from_url(url)
                if url_name:
                    name = url_name
                else:
                    console.print(
                        f"[yellow]âš  Skipping product {base_product_id}: Could not extract name[/yellow]"
                    )
                    return []

            description = api_data.get("description")
            all_colors = api_data.get("colors", [])

            # Create a product entry for each color variant
            for variant in api_data["color_variants"]:
                color_name = variant["color_name"]
                color_slug = slugify_color(color_name)

                # Create unique product ID for this color variant
                variant_product_id = f"{base_product_id}_{color_slug}"

                # Get variant-specific data (keep ALL images; storage limit applied in pipeline)
                variant_images = variant.get("images", [])
                variant_price = variant.get("price")
                variant_original_price = variant.get("original_price")
                variant_sizes = variant.get("sizes", [])

                # Skip variants without images
                if not variant_images and not variant_price:
                    console.print(
                        f"[yellow]âš  Skipping color variant {color_name}: No images and no price[/yellow]"
                    )
                    continue

                product_data = RawProductData(
                    product_id=variant_product_id,
                    name=name,
                    url=url,
                    category=category,
                    price_current=variant_price,
                    price_original=variant_original_price,
                    description=description,
                    colors=all_colors,  # All available colors (for reference)
                    color=color_name,  # This specific variant's color
                    parent_product_id=base_product_id,  # Link back to original product
                    sizes=variant_sizes,
                    materials=materials,
                    image_urls=variant_images,
                    composition=composition,
                    composition_structured=composition_structured,
                )

                products.append(product_data)
                console.print(
                    f"[green]âœ“ Extracted variant: {name} - {color_name} ({variant_product_id})[/green]"
                )

            console.print(
                f"[blue]ðŸ“¦ Created {len(products)} color variant(s) for {name}[/blue]"
            )
            return products

        except Exception as e:
            console.print(
                f"[bold red]Error extracting product variants {url}: {e}[/bold red]"
            )
            return []
        finally:
            await page.close()

    async def _get_product_from_api(self, product_id: str) -> Optional[dict]:
        """
        Get product data from Zara's ITXRest API.

        Returns dict with: name, price, original_price, description, colors, images
        """
        try:
            import httpx

            api_url = f"https://www.zara.com/itxrest/2/catalog/store/11719/product/{product_id}"

            headers = {
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "application/json, text/plain, */*",
                "Accept-Language": "en-US,en;q=0.9",
                "Referer": "https://www.zara.com/us/en/",
            }

            async with httpx.AsyncClient(follow_redirects=True) as client:
                response = await client.get(api_url, headers=headers, timeout=10)

                if response.status_code != 200:
                    return None

                data = response.json()
                result = {}

                # Extract name
                result["name"] = data.get("name", "")

                # Extract description - now in seo.description instead of detail
                if "seo" in data and data["seo"].get("description"):
                    result["description"] = data["seo"]["description"]
                elif "detail" in data:
                    # Fallback to old location
                    desc_parts = []
                    for key in ["description", "longDescription"]:
                        if data["detail"].get(key):
                            desc_parts.append(data["detail"][key])
                    result["description"] = " ".join(desc_parts) if desc_parts else None

                # Extract colors and images from first color variant
                if "detail" in data and "colors" in data["detail"]:
                    colors_data = data["detail"]["colors"]
                    if colors_data:
                        # Get color names
                        result["colors"] = [
                            c.get("name", "") for c in colors_data if c.get("name")
                        ]

                        # Get images from first color
                        first_color = colors_data[0]
                        if "xmedia" in first_color:
                            images = []
                            for media in first_color["xmedia"]:
                                # Use the deliveryUrl from extraInfo if available
                                if "extraInfo" in media and media["extraInfo"].get(
                                    "deliveryUrl"
                                ):
                                    img_url = media["extraInfo"]["deliveryUrl"]
                                    # Add width parameter for reasonable size
                                    if "?" in img_url:
                                        img_url += "&w=850"
                                    else:
                                        img_url += "?w=850"
                                    images.append(img_url)
                                elif media.get("path") and media.get("name"):
                                    # Fallback: Build the image URL from path/name
                                    img_url = f"https://static.zara.net/photos/{media['path']}/{media['name']}.jpg?w=850"
                                    images.append(img_url)
                            result["images"] = images  # All images

                        # Get price from first color's first size
                        if "sizes" in first_color and first_color["sizes"]:
                            first_size = first_color["sizes"][0]
                            if "price" in first_size:
                                # Price is in cents
                                result["price"] = first_size["price"] / 100
                            if "oldPrice" in first_size:
                                result["original_price"] = first_size["oldPrice"] / 100

                if result.get("name"):
                    console.print(
                        f"[dim]Got product data from API: {result.get('name')}[/dim]"
                    )
                    return result

        except Exception as e:
            console.print(f"[dim]API lookup failed: {e}[/dim]")

        return None

    async def _get_product_all_colors_from_api(self, product_id: str) -> Optional[dict]:
        """
        Get product data from Zara's ITXRest API with ALL color variants.

        Returns dict with:
            - name, description (shared)
            - colors: list of color names
            - color_variants: list of dicts, each with:
                - color_name, color_id, images, price, original_price, sizes
        """
        try:
            import httpx

            api_url = f"https://www.zara.com/itxrest/2/catalog/store/11719/product/{product_id}"

            headers = {
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "application/json, text/plain, */*",
                "Accept-Language": "en-US,en;q=0.9",
                "Referer": "https://www.zara.com/us/en/",
            }

            async with httpx.AsyncClient(follow_redirects=True) as client:
                response = await client.get(api_url, headers=headers, timeout=10)

                if response.status_code != 200:
                    return None

                data = response.json()
                result = {
                    "name": data.get("name", ""),
                    "colors": [],
                    "color_variants": [],
                }

                # Extract description - now in seo.description instead of detail
                if "seo" in data and data["seo"].get("description"):
                    result["description"] = data["seo"]["description"]
                elif "detail" in data:
                    # Fallback to old location
                    desc_parts = []
                    for key in ["description", "longDescription"]:
                        if data["detail"].get(key):
                            desc_parts.append(data["detail"][key])
                    result["description"] = " ".join(desc_parts) if desc_parts else None

                # Extract ALL color variants
                if "detail" in data and "colors" in data["detail"]:
                    colors_data = data["detail"]["colors"]
                    if colors_data:
                        for color_info in colors_data:
                            color_name = color_info.get("name", "")
                            if not color_name:
                                continue

                            result["colors"].append(color_name)

                            variant = {
                                "color_name": color_name,
                                "color_id": color_info.get("id", ""),
                                "images": [],
                                "price": None,
                                "original_price": None,
                                "sizes": [],
                            }

                            # Get images for this color
                            if "xmedia" in color_info:
                                for media in color_info["xmedia"]:
                                    if "extraInfo" in media and media["extraInfo"].get(
                                        "deliveryUrl"
                                    ):
                                        img_url = media["extraInfo"]["deliveryUrl"]
                                        if "?" in img_url:
                                            img_url += "&w=850"
                                        else:
                                            img_url += "?w=850"
                                        variant["images"].append(img_url)
                                    elif media.get("path") and media.get("name"):
                                        img_url = f"https://static.zara.net/photos/{media['path']}/{media['name']}.jpg?w=850"
                                        variant["images"].append(img_url)
                                # No limit - config.storage.max_images_per_product is applied later

                            # Get price and sizes for this color
                            if "sizes" in color_info and color_info["sizes"]:
                                for size_info in color_info["sizes"]:
                                    size_name = size_info.get("name", "")
                                    if size_name:
                                        # Check availability
                                        availability = size_info.get("availability", "")
                                        is_available = availability in [
                                            "in_stock",
                                            "low_on_stock",
                                            "coming_soon",
                                        ]
                                        variant["sizes"].append(
                                            {
                                                "size": size_name,
                                                "available": is_available,
                                                "availability": availability,
                                            }
                                        )

                                # Get price from first size
                                first_size = color_info["sizes"][0]
                                if "price" in first_size:
                                    variant["price"] = first_size["price"] / 100
                                if "oldPrice" in first_size:
                                    variant["original_price"] = (
                                        first_size["oldPrice"] / 100
                                    )

                            result["color_variants"].append(variant)

                if result.get("name") and result.get("color_variants"):
                    console.print(
                        f"[dim]Got {len(result['color_variants'])} color variant(s) from API: {result.get('name')}[/dim]"
                    )
                    return result

        except Exception as e:
            console.print(f"[dim]API color lookup failed: {e}[/dim]")

        return None

    def _extract_name_from_url(self, url: str) -> Optional[str]:
        """Extract a product name from the URL as a fallback."""
        try:
            # URL format: /us/en/product-name-here-p12345678.html
            match = re.search(r"/([^/]+)-p\d+\.html", url)
            if match:
                name_slug = match.group(1)
                # Convert slug to title case
                name = name_slug.replace("-", " ").title()
                return name
        except:
            pass
        return None

    def _extract_product_id(self, url: str) -> str:
        """Extract product ID from URL."""
        # Zara URLs are like: /us/en/product-name-p12345678.html
        match = re.search(r"-p(\d+)\.html", url)
        if match:
            return match.group(1)
        return url.split("/")[-1].replace(".html", "")

    async def _extract_text(self, page: Page, selectors: list[str]) -> Optional[str]:
        """Extract text from first matching selector."""
        for selector in selectors:
            try:
                element = await page.query_selector(selector)
                if element:
                    text = await element.text_content()
                    if text:
                        return text.strip()
            except:
                continue
        return None

    async def _extract_prices(
        self, page: Page
    ) -> tuple[Optional[float], Optional[float]]:
        """Extract current and original prices."""
        price_current = None
        price_original = None

        try:
            # Try to get price data from the page
            price_data = await page.evaluate(
                """
                () => {
                    const result = {current: null, original: null};

                    // Try current price selectors
                    const currentSelectors = [
                        '.money-amount__main',
                        '.price__amount--current',
                        '[data-qa="product-price"]',
                        '.product-detail-info__price .money-amount'
                    ];

                    for (const sel of currentSelectors) {
                        const el = document.querySelector(sel);
                        if (el) {
                            const text = el.textContent.trim();
                            const match = text.match(/[\\d.,]+/);
                            if (match) {
                                result.current = parseFloat(match[0].replace(',', ''));
                                break;
                            }
                        }
                    }

                    // Try original price (crossed out) selectors
                    const originalSelectors = [
                        '.price__amount--old',
                        '.money-amount--old',
                        '[class*="original-price"]',
                        'del .money-amount'
                    ];

                    for (const sel of originalSelectors) {
                        const el = document.querySelector(sel);
                        if (el) {
                            const text = el.textContent.trim();
                            const match = text.match(/[\\d.,]+/);
                            if (match) {
                                result.original = parseFloat(match[0].replace(',', ''));
                                break;
                            }
                        }
                    }

                    return result;
                }
            """
            )

            price_current = price_data.get("current")
            price_original = price_data.get("original")

        except Exception as e:
            console.print(f"[yellow]Warning: Could not extract prices: {e}[/yellow]")

        return price_current, price_original

    async def _extract_colors(self, page: Page) -> list[str]:
        """Extract available colors."""
        try:
            colors = await page.evaluate(
                """
                () => {
                    const colors = [];
                    const selectors = [
                        '.product-detail-color-selector__color-name',
                        '[class*="color-name"]',
                        '.product-detail-selected-color',
                    ];

                    for (const sel of selectors) {
                        const elements = document.querySelectorAll(sel);
                        for (const el of elements) {
                            const text = el.textContent.trim();
                            if (text && !colors.includes(text)) {
                                colors.push(text);
                            }
                        }
                        if (colors.length > 0) break;
                    }
                    return colors;
                }
            """
            )
            return colors
        except:
            return []

    async def _extract_sizes(self, page: Page) -> list[dict]:
        """Extract available sizes with availability status.

        Uses Zara's ITXRest API for reliable size data.
        Falls back to DOM extraction if API fails.
        Returns a list of dicts: [{"size": "M", "available": true}, ...]
        """
        try:
            import httpx

            # Get the product ID from the current URL
            url = page.url
            product_id = self._extract_product_id(url)

            if not product_id:
                console.print(
                    "[yellow]Could not extract product ID for size lookup[/yellow]"
                )
                return []

            # Query Zara's ITXRest API directly using httpx (not browser)
            # This bypasses any browser-based bot detection
            api_url = f"https://www.zara.com/itxrest/2/catalog/store/11719/product/{product_id}"

            headers = {
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "application/json, text/plain, */*",
                "Accept-Language": "en-US,en;q=0.9",
                "Referer": "https://www.zara.com/us/en/",
            }

            async with httpx.AsyncClient(follow_redirects=True) as client:
                response = await client.get(api_url, headers=headers, timeout=10)

                if response.status_code != 200:
                    console.print(
                        f"[yellow]Size API returned status {response.status_code}[/yellow]"
                    )
                    return []

                data = response.json()
                sizes = []
                seen_sizes = set()

                # Extract sizes from the first color variant
                if "detail" in data and "colors" in data["detail"]:
                    colors = data["detail"]["colors"]
                    if colors:
                        first_color = colors[0]
                        if "sizes" in first_color:
                            for size in first_color["sizes"]:
                                size_name = size.get("name", "")
                                if size_name and size_name not in seen_sizes:
                                    availability = size.get("availability", "unknown")
                                    sizes.append(
                                        {
                                            "size": size_name,
                                            "available": availability
                                            in ("in_stock", "low_on_stock"),
                                            "availability": availability,
                                            "sku": size.get("sku"),
                                        }
                                    )
                                    seen_sizes.add(size_name)

                if sizes:
                    console.print(
                        f"[dim]Found {len(sizes)} sizes via API: {[s['size'] for s in sizes]}[/dim]"
                    )

                return sizes

        except Exception as e:
            console.print(
                f"[yellow]Warning: Could not extract sizes via API: {e}[/yellow]"
            )
            return []

    async def _extract_materials(self, page: Page) -> list[str]:
        """Extract material/composition information."""
        try:
            materials = await page.evaluate(
                """
                () => {
                    const materials = [];
                    const selectors = [
                        '.product-detail-info__composition li',
                        '[class*="composition"] li',
                        '.structured-component-text-block-paragraph span',
                    ];

                    for (const sel of selectors) {
                        const elements = document.querySelectorAll(sel);
                        for (const el of elements) {
                            const text = el.textContent.trim();
                            if (text && text.includes('%') && !materials.includes(text)) {
                                materials.push(text);
                            }
                        }
                        if (materials.length > 0) break;
                    }
                    return materials;
                }
            """
            )
            return materials
        except:
            return []

    async def _extract_composition(
        self, page: Page, product_id: str, category: Optional[str] = None
    ) -> tuple[Optional[str], Optional[dict]]:
        """
        Extract composition/material information from the product page.

        This looks for the "COMPOSITION & CARE" section and extracts the fabric
        composition (e.g., "100% cotton" or "49% polyamide, 29% polyester, 14% acrylic, 8% wool").

        Args:
            page: Playwright page
            product_id: Product ID for API fallback

        Returns:
            Tuple of (legacy string composition, structured composition dict)
            Structured format: {
                "parts": [
                    {
                        "name": "OUTER SHELL" or "UPPER" etc,
                        "areas": [
                            {
                                "name": "MAIN FABRIC" (optional),
                                "components": [{"material": "cotton", "percentage": "82%"}, ...]
                            }
                        ]
                    }
                ]
            }
        """
        composition_string = None
        composition_structured = None

        # First, try to get composition from the API
        try:
            import httpx

            api_url = f"https://www.zara.com/itxrest/2/catalog/store/11719/product/{product_id}"

            headers = {
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "application/json, text/plain, */*",
                "Accept-Language": "en-US,en;q=0.9",
                "Referer": "https://www.zara.com/us/en/",
            }

            async with httpx.AsyncClient(follow_redirects=True) as client:
                response = await client.get(api_url, headers=headers, timeout=10)

                if response.status_code == 200:
                    data = response.json()

                    # Check for detailedComposition at the detail level first (most reliable)
                    if "detail" in data:
                        detail = data["detail"]
                        if "detailedComposition" in detail:
                            detailed_comp = detail["detailedComposition"]
                            if detailed_comp and isinstance(detailed_comp, dict):
                                # Parse the structured composition
                                structured_parts = []
                                flat_components = []

                                parts = detailed_comp.get("parts", [])
                                for part in parts:
                                    if isinstance(part, dict):
                                        part_name = part.get("description", "")
                                        areas = part.get("areas", [])
                                        components = part.get("components", [])

                                        structured_part = {
                                            "name": part_name,
                                            "areas": [],
                                        }

                                        # Handle areas (sub-sections like "MAIN FABRIC", "SECONDARY FABRIC")
                                        if areas:
                                            for area in areas:
                                                if isinstance(area, dict):
                                                    area_name = area.get(
                                                        "description", ""
                                                    )
                                                    area_components = area.get(
                                                        "components", []
                                                    )

                                                    area_data = {
                                                        "name": area_name,
                                                        "components": [],
                                                    }

                                                    for comp in area_components:
                                                        if isinstance(comp, dict):
                                                            material = comp.get(
                                                                "material", ""
                                                            )
                                                            percentage = comp.get(
                                                                "percentage", ""
                                                            )
                                                            if material and percentage:
                                                                area_data[
                                                                    "components"
                                                                ].append(
                                                                    {
                                                                        "material": material,
                                                                        "percentage": percentage,
                                                                    }
                                                                )
                                                                flat_components.append(
                                                                    f"{percentage} {material}"
                                                                )

                                                    if area_data["components"]:
                                                        structured_part["areas"].append(
                                                            area_data
                                                        )

                                        # Handle direct components (no sub-areas)
                                        if components:
                                            direct_area = {
                                                "name": "",  # No sub-area name
                                                "components": [],
                                            }
                                            for comp in components:
                                                if isinstance(comp, dict):
                                                    material = comp.get("material", "")
                                                    percentage = comp.get(
                                                        "percentage", ""
                                                    )
                                                    if material and percentage:
                                                        direct_area[
                                                            "components"
                                                        ].append(
                                                            {
                                                                "material": material,
                                                                "percentage": percentage,
                                                            }
                                                        )
                                                        flat_components.append(
                                                            f"{percentage} {material}"
                                                        )

                                            if direct_area["components"]:
                                                structured_part["areas"].append(
                                                    direct_area
                                                )

                                        if structured_part["areas"]:
                                            structured_parts.append(structured_part)

                                if structured_parts:
                                    # Shoes/boots: keep only UPPER (no lining, sole, insole)
                                    if category and (category.lower() in ("shoes", "boots")):
                                        upper_parts = [
                                            p
                                            for p in structured_parts
                                            if (p.get("name") or "").upper().strip() == "UPPER"
                                        ]
                                        if upper_parts:
                                            composition_structured = {"parts": upper_parts}
                                            flat_components = []
                                            for p in upper_parts:
                                                for area in p.get("areas", []):
                                                    for c in area.get("components", []):
                                                        flat_components.append(
                                                            f"{c.get('percentage', '')} {c.get('material', '')}".strip()
                                                        )
                                            composition_string = ", ".join(flat_components)
                                            console.print(
                                                f"[dim]Got composition (UPPER only for {category}): {composition_string}[/dim]"
                                            )
                                            return composition_string, composition_structured
                                    composition_structured = {"parts": structured_parts}
                                    composition_string = ", ".join(flat_components)
                                    console.print(
                                        f"[dim]Got structured composition from API: {len(structured_parts)} part(s)[/dim]"
                                    )
                                    return composition_string, composition_structured

                    # Look for composition in colors array
                    if "detail" in data and "colors" in data["detail"]:
                        colors = data["detail"]["colors"]
                        if colors:
                            first_color = colors[0]

                            # Try rawMaterials first
                            if "rawMaterials" in first_color:
                                raw_materials = first_color["rawMaterials"]
                                if isinstance(raw_materials, list) and raw_materials:
                                    comp_parts = []
                                    for mat in raw_materials:
                                        if isinstance(mat, dict):
                                            percentage = mat.get("percentage", "")
                                            material = mat.get(
                                                "description",
                                                mat.get(
                                                    "name", mat.get("material", "")
                                                ),
                                            )
                                            if percentage and material:
                                                comp_parts.append(
                                                    f"{percentage}% {material}"
                                                )
                                            elif material:
                                                comp_parts.append(material)
                                        elif isinstance(mat, str):
                                            comp_parts.append(mat)
                                    if comp_parts:
                                        composition_string = ", ".join(comp_parts)
                                        console.print(
                                            f"[dim]Got composition from API rawMaterials: {composition_string}[/dim]"
                                        )
                                        return composition_string, None
                                elif isinstance(raw_materials, str):
                                    composition_string = raw_materials
                                    console.print(
                                        f"[dim]Got composition from API rawMaterials: {composition_string}[/dim]"
                                    )
                                    return composition_string, None

                            # Try composition field directly
                            if "composition" in first_color:
                                comp = first_color["composition"]
                                if isinstance(comp, str) and comp:
                                    console.print(
                                        f"[dim]Got composition from API composition field: {comp}[/dim]"
                                    )
                                    return comp, None

                            # Try materials field
                            if "materials" in first_color:
                                materials = first_color["materials"]
                                if isinstance(materials, list) and materials:
                                    comp_parts = []
                                    for mat in materials:
                                        if isinstance(mat, dict):
                                            parts_list = mat.get("parts", [])
                                            for part in parts_list:
                                                if isinstance(part, dict):
                                                    percentage = part.get(
                                                        "percentage", ""
                                                    )
                                                    material = part.get(
                                                        "description",
                                                        part.get("name", ""),
                                                    )
                                                    if percentage and material:
                                                        comp_parts.append(
                                                            f"{percentage}% {material}"
                                                        )
                                        elif isinstance(mat, str):
                                            comp_parts.append(mat)
                                    if comp_parts:
                                        composition_string = ", ".join(comp_parts)
                                        console.print(
                                            f"[dim]Got composition from API materials: {composition_string}[/dim]"
                                        )
                                        return composition_string, None

                    # Try detail.composition or detail.rawMaterials at the top level
                    if "detail" in data:
                        detail = data["detail"]
                        if "composition" in detail:
                            comp = detail["composition"]
                            if isinstance(comp, str) and comp:
                                console.print(
                                    f"[dim]Got composition from API detail.composition: {comp}[/dim]"
                                )
                                return comp, None
                        if "rawMaterials" in detail:
                            comp = detail["rawMaterials"]
                            if isinstance(comp, str) and comp:
                                console.print(
                                    f"[dim]Got composition from API detail.rawMaterials: {comp}[/dim]"
                                )
                                return comp, None

        except Exception as e:
            console.print(f"[dim]API composition lookup failed: {e}[/dim]")

        # Fallback: Try to click "COMPOSITION & CARE" and extract from expanded content
        try:
            # Try to find and click the composition toggle button
            composition_buttons = await page.query_selector_all(
                'button:has-text("COMPOSITION"), button:has-text("Composition")'
            )
            for btn in composition_buttons:
                try:
                    await btn.click()
                    await asyncio.sleep(0.5)  # Wait for content to expand
                except:
                    pass
        except:
            pass

        # Now try to extract composition from the DOM
        try:
            composition = await page.evaluate(
                """
                () => {
                    // Strategy 1: Look for elements that contain composition percentages
                    const selectors = [
                        // Zara composition sections (after clicking expand)
                        '.product-detail-extra-detail .structured-component-text-block-paragraph',
                        '.product-detail-composition',
                        '[data-component-type="composition"]',
                        '.product-detail-info__composition',
                        '.product-detail-view__main-info-inner p',
                        // Composition in expandable sections
                        '.expandable-text__inner-content',
                        '.product-detail-extra-detail__content',
                        // General composition patterns
                        '[class*="composition"] p',
                        '[class*="composition"] span',
                        '[class*="material"] p',
                        // Text blocks that might contain composition
                        '.expandable-text__content p',
                        '.product-detail-extra-detail p',
                        // Look for any p tags with composition info
                        'p',
                        'span',
                    ];

                    for (const sel of selectors) {
                        const elements = document.querySelectorAll(sel);
                        for (const el of elements) {
                            const text = el.textContent.trim();
                            // Look for text containing percentage signs (likely composition)
                            if (text && text.includes('%') && /\\d+%/.test(text)) {
                                // Must have at least one material keyword
                                const materialKeywords = ['cotton', 'polyester', 'wool', 'silk', 'linen', 'nylon',
                                    'polyamide', 'acrylic', 'viscose', 'elastane', 'spandex', 'rayon', 'cashmere',
                                    'leather', 'denim', 'modal', 'lyocell', 'tencel'];
                                const textLower = text.toLowerCase();
                                const hasMaterial = materialKeywords.some(kw => textLower.includes(kw));

                                if (hasMaterial) {
                                    // Clean up the text
                                    const cleaned = text.replace(/\\s+/g, ' ').trim();
                                    if (cleaned.length > 3 && cleaned.length < 500) {
                                        return cleaned;
                                    }
                                }
                            }
                        }
                    }

                    // Strategy 2: Search the full page text for composition patterns
                    const pageText = document.body.innerText;

                    // Look for "Composition:" followed by percentage info
                    const compositionMatch = pageText.match(/Composition:?\\s*([\\d]+%[^.\\n]*(?:,\\s*\\d+%[^.\\n]*)*)/i);
                    if (compositionMatch) {
                        return compositionMatch[1].trim();
                    }

                    // Look for patterns like "100% cotton" or "60% cotton, 40% polyester"
                    const materialKeywords = ['cotton', 'polyester', 'wool', 'silk', 'linen', 'nylon',
                        'polyamide', 'acrylic', 'viscose', 'elastane', 'spandex', 'rayon', 'cashmere',
                        'leather', 'denim', 'modal', 'lyocell', 'tencel'];
                    const materialPattern = pageText.match(/\\d+%\\s*[a-zA-Z]+(?:\\s*,\\s*\\d+%\\s*[a-zA-Z]+)*/gi);
                    if (materialPattern) {
                        // Filter to only matches that contain material keywords
                        const validMatches = materialPattern.filter(m => {
                            const lower = m.toLowerCase();
                            return materialKeywords.some(kw => lower.includes(kw));
                        });
                        if (validMatches.length > 0) {
                            // Find the most complete match (longest)
                            const best = validMatches.sort((a, b) => b.length - a.length)[0];
                            if (best && best.length < 200) {
                                return best.trim();
                            }
                        }
                    }

                    return null;
                }
            """
            )

            if composition:
                console.print(f"[dim]Got composition from DOM: {composition}[/dim]")

        except Exception as e:
            console.print(f"[dim]DOM composition extraction failed: {e}[/dim]")

        return composition, None  # Return tuple (string, None for structured)

    async def _extract_images(self, page: Page) -> list[str]:
        """Extract product image URLs."""
        image_urls = []

        try:
            # First, scroll through the page to trigger lazy loading
            await self._scroll_page(page, scroll_count=5)
            await asyncio.sleep(2)

            # Try to extract images using multiple strategies
            image_urls = await page.evaluate(
                """
                () => {
                    const images = new Set();

                    // Strategy 1: Look for ALL img tags and check for product images
                    document.querySelectorAll('img').forEach(img => {
                        const src = img.src || '';
                        const srcset = img.srcset || '';
                        const dataSrc = img.getAttribute('data-src') || '';

                        // Check all possible sources
                        [src, dataSrc].forEach(url => {
                            if (url && (url.includes('static.zara') || url.includes('zara.com')) &&
                                !url.includes('transparent') && !url.includes('placeholder') &&
                                !url.includes('logo') && !url.includes('icon')) {
                                images.add(url.split('?')[0]);
                            }
                        });

                        // Check srcset for higher res
                        if (srcset) {
                            const sources = srcset.split(',').map(s => s.trim().split(' ')[0]);
                            sources.forEach(url => {
                                if (url && (url.includes('static.zara') || url.includes('zara.com')) &&
                                    !url.includes('transparent') && !url.includes('placeholder')) {
                                    images.add(url.split('?')[0]);
                                }
                            });
                        }
                    });

                    // Strategy 2: Look for picture elements with source tags
                    document.querySelectorAll('picture source').forEach(source => {
                        const srcset = source.srcset || '';
                        if (srcset) {
                            const sources = srcset.split(',').map(s => s.trim().split(' ')[0]);
                            sources.forEach(url => {
                                if (url && (url.includes('static.zara') || url.includes('zara.com')) &&
                                    !url.includes('transparent')) {
                                    images.add(url.split('?')[0]);
                                }
                            });
                        }
                    });

                    // Strategy 3: Check all elements for background-image styles
                    document.querySelectorAll('*').forEach(el => {
                        const style = window.getComputedStyle(el);
                        const bgImage = style.backgroundImage;
                        if (bgImage && bgImage !== 'none') {
                            const match = bgImage.match(/url\\(['"]?(https?:[^'"\\)]+)['"]?\\)/);
                            if (match && match[1] && (match[1].includes('static.zara') || match[1].includes('zara.com'))) {
                                images.add(match[1].split('?')[0]);
                            }
                        }
                    });

                    // Strategy 4: Look in data attributes across all elements
                    document.querySelectorAll('[data-src], [data-srcset], [data-image], [data-original]').forEach(el => {
                        ['data-src', 'data-srcset', 'data-image', 'data-original'].forEach(attr => {
                            const value = el.getAttribute(attr) || '';
                            if (value && (value.includes('static.zara') || value.includes('zara.com'))) {
                                if (value.includes(',')) {
                                    value.split(',').forEach(url => {
                                        const cleanUrl = url.trim().split(' ')[0];
                                        if (cleanUrl) images.add(cleanUrl.split('?')[0]);
                                    });
                                } else {
                                    images.add(value.split('?')[0]);
                                }
                            }
                        });
                    });

                    // Strategy 5: Search the page HTML for image URLs as a fallback
                    const html = document.documentElement.innerHTML;
                    const urlPattern = /https?:\\/\\/static\\.zara\\.net\\/photos[^"'\\s)]+\\.jpg/gi;
                    const matches = html.match(urlPattern) || [];
                    matches.forEach(url => images.add(url.split('?')[0]));

                    // Filter results
                    const filtered = Array.from(images).filter(url => {
                        // Must be a reasonable image URL
                        return url.length > 20 &&
                               (url.endsWith('.jpg') || url.endsWith('.png') || url.endsWith('.webp') ||
                                url.includes('/w/') || url.includes('/photos/'));
                    });

                    return filtered;
                }
            """
            )

            console.print(f"[dim]Found {len(image_urls)} image URLs from DOM[/dim]")

        except Exception as e:
            console.print(f"[yellow]Warning: DOM image extraction failed: {e}[/yellow]")

        # If no images found, try to get them from network requests
        if not image_urls:
            console.print(
                "[dim]Trying to capture images from page screenshots...[/dim]"
            )
            # Take a screenshot as fallback - at least we have something
            try:
                screenshot_path = (
                    f"/tmp/zara_debug_{datetime.now().strftime('%H%M%S')}.png"
                )
                await page.screenshot(path=screenshot_path, full_page=True)
                console.print(f"[dim]Debug screenshot saved: {screenshot_path}[/dim]")
            except:
                pass

        # Return all image URLs (storage limit applied in pipeline when saving)
        return image_urls

    async def extract_all_products(self) -> list[RawProductData]:
        """
        Extract products from all configured categories.

        Returns:
            List of RawProductData objects
        """
        all_products = []

        for category_key in self.config.categories.keys():
            console.print(
                f"\n[bold magenta]Processing category: {category_key}[/bold magenta]"
            )

            # Get product URLs
            product_urls = await self.get_category_product_urls(category_key)

            # Extract each product
            for url in product_urls:
                await self._random_delay()
                product = await self.extract_product(url, category_key)
                if product:
                    all_products.append(product)

        console.print(
            f"\n[bold green]Extracted {len(all_products)} products total[/bold green]"
        )
        return all_products


async def main():
    """Test the extractor."""
    async with ZaraExtractor() as extractor:
        products = await extractor.extract_all_products()
        for p in products:
            console.print(
                f"  - {p.name}: ${p.price_current} ({len(p.image_urls)} images)"
            )


if __name__ == "__main__":
    asyncio.run(main())
